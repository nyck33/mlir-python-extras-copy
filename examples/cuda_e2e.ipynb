{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Based on [transform-mma-sync-matmul-f16-f16-accum.mlir](https://github.com/llvm/llvm-project/blob/9cc2122bf5a81f7063c2a32b2cb78c8d615578a1/mlir/test/Integration/GPU/CUDA/TensorCore/sm80/transform-mma-sync-matmul-f16-f16-accum.mlir#L6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from readme\n",
    "\n",
    "```shell\n",
    "$ pip install mlir-python-bindings -f https://makslevental.github.io/wheels/\n",
    "```\n",
    "\n",
    "and then\n",
    "\n",
    "```shell\n",
    "$ pip install git+https://github.com/makslevental/mlir-python-extras\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "\u001b[31mERROR: Could not find a version that satisfies the requirement mlir_python_bindings==19.0.0.2024020206+cuda.374a600d (from versions: 19.0.0.2024022801+bcbce807, 19.0.0.2024022801+cuda.bcbce807, 19.0.0.2024022801+openmp.bcbce807, 19.0.0.2024022801+vulkan.bcbce807, 19.0.0.2024022901+0fe4b9da, 19.0.0.2024022901+cuda.0fe4b9da, 19.0.0.2024022901+openmp.0fe4b9da, 19.0.0.2024022901+vulkan.0fe4b9da, 19.0.0.2024030101+7ceb74f5, 19.0.0.2024030101+cuda.7ceb74f5, 19.0.0.2024030101+openmp.7ceb74f5, 19.0.0.2024030101+vulkan.7ceb74f5, 19.0.0.2024030217+b901b0d3, 19.0.0.2024030217+cuda.b901b0d3, 19.0.0.2024030217+openmp.b901b0d3, 19.0.0.2024030217+vulkan.b901b0d3, 19.0.0.2024030301+4dd9c2ed, 19.0.0.2024030301+cuda.4dd9c2ed, 19.0.0.2024030301+openmp.4dd9c2ed, 19.0.0.2024030301+vulkan.4dd9c2ed, 19.0.0.2024030401+5f058aa2, 19.0.0.2024030401+cuda.5f058aa2, 19.0.0.2024030401+openmp.5f058aa2, 19.0.0.2024030401+vulkan.5f058aa2, 19.0.0.2024030501+ccf0c8da, 19.0.0.2024030501+cuda.ccf0c8da, 19.0.0.2024030501+openmp.ccf0c8da, 19.0.0.2024030501+vulkan.ccf0c8da, 19.0.0.2024030601+11f74cd4, 19.0.0.2024030601+cuda.11f74cd4, 19.0.0.2024030601+openmp.11f74cd4, 19.0.0.2024030601+vulkan.11f74cd4, 19.0.0.2024030701+318bff68, 19.0.0.2024030701+cuda.318bff68, 19.0.0.2024030701+openmp.318bff68, 19.0.0.2024030701+vulkan.318bff68, 19.0.0.2024030801+cuda.e7a22e72, 19.0.0.2024030801+e7a22e72, 19.0.0.2024030801+openmp.e7a22e72, 19.0.0.2024030801+vulkan.e7a22e72, 19.0.0.2024030901+6b270358, 19.0.0.2024030901+cuda.6b270358, 19.0.0.2024030901+openmp.6b270358, 19.0.0.2024030901+vulkan.6b270358, 19.0.0.2024031001+15e94781, 19.0.0.2024031001+cuda.15e94781, 19.0.0.2024031001+openmp.15e94781, 19.0.0.2024031001+vulkan.15e94781, 19.0.0.2024031101+cuda.edd4c6c6, 19.0.0.2024031101+edd4c6c6, 19.0.0.2024031101+openmp.edd4c6c6, 19.0.0.2024031101+vulkan.edd4c6c6, 19.0.0.2024031203+cuda.e4a54675, 19.0.0.2024031203+e4a54675, 19.0.0.2024031203+openmp.e4a54675, 19.0.0.2024031203+vulkan.e4a54675, 19.0.0.2024031217+93503aaf, 19.0.0.2024031217+cuda.93503aaf, 19.0.0.2024031217+openmp.93503aaf, 19.0.0.2024031217+vulkan.93503aaf, 19.0.0.2024031301+9d6c43b4, 19.0.0.2024031301+cuda.9d6c43b4, 19.0.0.2024031301+openmp.9d6c43b4, 19.0.0.2024031301+vulkan.9d6c43b4, 19.0.0.2024031401+02cadde5, 19.0.0.2024031401+cuda.02cadde5, 19.0.0.2024031401+openmp.02cadde5, 19.0.0.2024031401+vulkan.02cadde5, 19.0.0.2024031501+cuda.e4f71959, 19.0.0.2024031501+e4f71959, 19.0.0.2024031501+openmp.e4f71959, 19.0.0.2024031501+vulkan.e4f71959, 19.0.0.2024031601+8386a388, 19.0.0.2024031601+cuda.8386a388, 19.0.0.2024031601+openmp.8386a388, 19.0.0.2024031601+vulkan.8386a388, 19.0.0.2024031701+8f878c50, 19.0.0.2024031701+cuda.8f878c50, 19.0.0.2024031701+openmp.8f878c50, 19.0.0.2024031701+vulkan.8f878c50, 19.0.0.2024031801+6cc8d54a, 19.0.0.2024031801+cuda.6cc8d54a, 19.0.0.2024031801+openmp.6cc8d54a, 19.0.0.2024031801+vulkan.6cc8d54a, 19.0.0.2024031901+a6296214, 19.0.0.2024031901+cuda.a6296214, 19.0.0.2024031901+openmp.a6296214, 19.0.0.2024031901+vulkan.a6296214, 19.0.0.2024032001+09db84cc, 19.0.0.2024032001+cuda.09db84cc, 19.0.0.2024032001+openmp.09db84cc, 19.0.0.2024032001+vulkan.09db84cc, 19.0.0.2024032101+631248dc, 19.0.0.2024032101+cuda.631248dc, 19.0.0.2024032101+openmp.631248dc, 19.0.0.2024032101+vulkan.631248dc, 19.0.0.2024032201+718fbbef, 19.0.0.2024032201+cuda.718fbbef, 19.0.0.2024032201+openmp.718fbbef, 19.0.0.2024032201+vulkan.718fbbef, 19.0.0.2024032301+2f6b1b4b, 19.0.0.2024032301+cuda.2f6b1b4b, 19.0.0.2024032301+openmp.2f6b1b4b, 19.0.0.2024032301+vulkan.2f6b1b4b, 19.0.0.2024032401+8e698a1d, 19.0.0.2024032401+cuda.8e698a1d, 19.0.0.2024032401+openmp.8e698a1d, 19.0.0.2024032401+vulkan.8e698a1d, 19.0.0.2024032501+cceedc93, 19.0.0.2024032501+cuda.cceedc93, 19.0.0.2024032601+cuda.ce73b167, 19.0.0.2024032601+openmp.ce73b167, 19.0.0.2024032601+vulkan.ce73b167, 19.0.0.2024032701+4720e383, 19.0.0.2024032701+cuda.4720e383, 19.0.0.2024032701+openmp.4720e383, 19.0.0.2024032701+vulkan.4720e383, 19.0.0.2024032801+1095f71b, 19.0.0.2024032801+cuda.1095f71b, 19.0.0.2024032801+openmp.1095f71b, 19.0.0.2024032801+vulkan.1095f71b, 19.0.0.2024032901+39e81374, 19.0.0.2024032901+cuda.39e81374, 19.0.0.2024032901+openmp.39e81374, 19.0.0.2024032901+vulkan.39e81374)\u001b[0m\u001b[31m\n",
    "\u001b[0m\u001b[31mERROR: No matching distribution found for mlir_python_bindings==19.0.0.2024020206+cuda.374a600d\u001b[0m\u001b[31m\n",
    "\u001b[0m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install mlir_python_bindings==19.0.0.2024032901+cuda.39e81374 -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032801+cuda.1095f71b -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032701+cuda.4720e383 -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032601+cuda.ce73b167 -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032501+cuda.cceedc93 -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032401+cuda.8e698a1d -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032301+cuda.2f6b1b4b -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032201+cuda.718fbbef -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032101+cuda.631248dc -f https://makslevental.github.io/wheels/\n",
    "pip install mlir_python_bindings==19.0.0.2024032001+cuda.09db84cc -f https://makslevental.github.io/wheels/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.587406187Z",
     "start_time": "2024-02-02T23:42:30.667121274Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh-QUDWiX-FD",
    "outputId": "6865a63a-daa4-4610-e33a-721d37c0211f"
   },
   "outputs": [],
   "source": [
    "# install cuda from above\n",
    "\n",
    "#! pip install -q git+https://github.com/makslevental/mlir-python-extras.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSATAYhg7pSZ"
   },
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.631298039Z",
     "start_time": "2024-02-02T23:42:40.630713260Z"
    },
    "id": "_R-_0M5ZYO8p"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mlir.extras.types as T\n",
    "from mlir.dialects import builtin\n",
    "from mlir.dialects.transform import * #any_op_t\n",
    "from mlir.dialects.transform.extras import * #named_sequence\n",
    "from mlir.dialects.transform.structured import MatchInterfaceEnum\n",
    "from mlir.ir import StringAttr, UnitAttr\n",
    "\n",
    "from mlir import _mlir_libs\n",
    "from mlir.extras.ast.canonicalize import canonicalize\n",
    "from mlir.extras.context import RAIIMLIRContext, ExplicitlyManagedModule\n",
    "from mlir.extras.dialects.ext import arith, memref, scf, gpu\n",
    "from mlir.extras.dialects.ext import linalg\n",
    "from mlir.extras.dialects.ext import transform\n",
    "from mlir.extras.dialects.ext.func import func\n",
    "from mlir.extras.runtime.passes import Pipeline, run_pipeline\n",
    "from mlir.extras.runtime.refbackend import LLVMJITBackend\n",
    "from mlir.extras.util import find_ops\n",
    "\n",
    "CUDA_RUNTIME_LIB_PATH = Path(_mlir_libs.__file__).parent / f\"libmlir_cuda_runtime.so\"\n",
    "assert CUDA_RUNTIME_LIB_PATH.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-JTcrjo7tNK"
   },
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.631765526Z",
     "start_time": "2024-02-02T23:42:40.630962853Z"
    },
    "id": "AGpWj9BzZLC_"
   },
   "outputs": [],
   "source": [
    "ctx = RAIIMLIRContext()\n",
    "module = ExplicitlyManagedModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGcDtgkv71YB"
   },
   "source": [
    "# Kernel and helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.728400651Z",
     "start_time": "2024-02-02T23:42:40.631097458Z"
    },
    "id": "7oQk4xJd72FI"
   },
   "outputs": [],
   "source": [
    "range_ = scf.range_\n",
    "\n",
    "M, K, N = 16, 16, 8\n",
    "\n",
    "# forward reference...\n",
    "# TODO(max): figure out closures...\n",
    "printMemrefF32_ = []\n",
    "\n",
    "\n",
    "@func\n",
    "def compute_linspace_val(ridx: T.index(), cidx: T.index(), stride_cidx: T.index()):\n",
    "    r = arith.index_cast(ridx, to=T.i32())\n",
    "    c = arith.index_cast(cidx, to=T.i32())\n",
    "    stride_c = arith.index_cast(stride_cidx, to=T.i32())\n",
    "    v2 = r * stride_c\n",
    "    v3 = c + v2\n",
    "    v4 = arith.sitofp(T.f16(), v3)\n",
    "    factor = arith.constant(64.0, T.f16())\n",
    "    v5 = arith.divf(v4, factor)\n",
    "    return v5\n",
    "\n",
    "\n",
    "@func\n",
    "@canonicalize(using=scf.canonicalizer)\n",
    "def print_lhs_as_memref_32(lhs: T.memref(M, K, T.f16())):\n",
    "    M = memref.dim(lhs, 0)\n",
    "    K = memref.dim(lhs, 1)\n",
    "    tmp_alloc = memref.alloc(M, K, T.f32())\n",
    "    for m in range_(0, M):\n",
    "        for k in range_(0, K):\n",
    "            f16 = lhs[m, k]\n",
    "            f32 = arith.extf(T.f32(), f16)\n",
    "            tmp_alloc[m, k] = f32\n",
    "\n",
    "    casted = memref.cast(T.memref(T.f32()), tmp_alloc)\n",
    "    printMemrefF32_[0](casted)\n",
    "    memref.dealloc(tmp_alloc)\n",
    "\n",
    "\n",
    "@func\n",
    "@canonicalize(using=scf.canonicalizer)\n",
    "def print_rhs_as_memref_32(rhs: T.memref(K, N, T.f16())):\n",
    "    K = memref.dim(rhs, 0)\n",
    "    N = memref.dim(rhs, 1)\n",
    "    tmp_alloc = memref.alloc(K, N, T.f32())\n",
    "    for k in range_(0, K):\n",
    "        for n in range_(0, N):\n",
    "            f16 = rhs[k, n]\n",
    "            f32 = arith.extf(T.f32(), f16)\n",
    "            tmp_alloc[k, n] = f32\n",
    "\n",
    "    casted = memref.cast(T.memref(T.f32()), tmp_alloc)\n",
    "    printMemrefF32_[0](casted)\n",
    "    memref.dealloc(tmp_alloc)\n",
    "\n",
    "\n",
    "@func\n",
    "@canonicalize(using=scf.canonicalizer)\n",
    "def print_res_as_memref_32(res: T.memref(M, N, T.f16())):\n",
    "    c0 = arith.constant(0, index=True)\n",
    "    c1 = arith.constant(1, index=True)\n",
    "    M = memref.dim(res, c0)\n",
    "    N = memref.dim(res, c1)\n",
    "    tmp_alloc = memref.alloc(M, N, T.f32())\n",
    "    for m in range_(0, M):\n",
    "        for n in range_(0, N):\n",
    "            f16 = res[m, n]\n",
    "            f32 = arith.extf(T.f32(), f16)\n",
    "            tmp_alloc[m, n] = f32\n",
    "\n",
    "    casted = memref.cast(T.memref(T.f32()), tmp_alloc)\n",
    "    printMemrefF32_[0](casted)\n",
    "    memref.dealloc(tmp_alloc)\n",
    "\n",
    "\n",
    "@func\n",
    "@canonicalize(using=scf.canonicalizer)\n",
    "def main():\n",
    "    lhs = memref.alloc(M, K, T.f16())\n",
    "    rhs = memref.alloc(K, N, T.f16())\n",
    "    res = memref.alloc(M, N, T.f16())\n",
    "\n",
    "    M_ = memref.dim(res, 0)\n",
    "    N_ = memref.dim(res, 1)\n",
    "    K_ = memref.dim(lhs, 1)\n",
    "\n",
    "    _f1 = arith.constant(1.0e00, T.f16())\n",
    "    _f0 = arith.constant(0.0e00, T.f16())\n",
    "    _c32 = arith.constant(32, T.index())\n",
    "\n",
    "    # Initialize the lhs matrix with a linspace function.\n",
    "    for r in range_(0, M_):\n",
    "        for c in range_(0, K_):\n",
    "            idx = compute_linspace_val(r, c, K_)\n",
    "            lhs[r, c] = idx\n",
    "\n",
    "    # Initialize the rhs matrix with a linspace function.\n",
    "    for r in range_(0, K_):\n",
    "        for c in range_(0, N_):\n",
    "            idx = compute_linspace_val(r, c, N_)\n",
    "            rhs[r, c] = idx\n",
    "\n",
    "    # Initialize the res matrix with a linspace function.\n",
    "    for r in range_(0, M_):\n",
    "        for c in range_(0, N_):\n",
    "            idx = compute_linspace_val(r, c, N_)\n",
    "            res[r, c] = idx\n",
    "\n",
    "    ulhs = memref.cast(T.memref(T.f16()), lhs)\n",
    "    urhs = memref.cast(T.memref(T.f16()), rhs)\n",
    "    ures = memref.cast(T.memref(T.f16()), res)\n",
    "    gpu.host_register(ulhs)\n",
    "    gpu.host_register(urhs)\n",
    "    gpu.host_register(ures)\n",
    "\n",
    "    print_lhs_as_memref_32(lhs)\n",
    "    print_rhs_as_memref_32(rhs)\n",
    "\n",
    "    @gpu.launch(grid_size=[1, 1, 1], block_size=[32, 1, 1])\n",
    "    def kernel(bx, by, bz, tx, ty, tz, *grid_block_sizes):\n",
    "        linalg.matmul(lhs, rhs, res)\n",
    "\n",
    "    print_res_as_memref_32(res)\n",
    "\n",
    "\n",
    "@builtin.module(attrs={\"transform.target_tag\": StringAttr.get(\"payload\")})\n",
    "def payload():\n",
    "    compute_linspace_val.emit()\n",
    "\n",
    "    @func\n",
    "    def printMemrefF32(x: T.memref(T.f32())):\n",
    "        ...\n",
    "\n",
    "    printMemrefF32_.append(printMemrefF32)\n",
    "\n",
    "    print_lhs_as_memref_32.emit()\n",
    "    print_rhs_as_memref_32.emit()\n",
    "    print_res_as_memref_32.emit()\n",
    "    main.emit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0vJZrpR74KB"
   },
   "source": [
    "# Transform schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.733574415Z",
     "start_time": "2024-02-02T23:42:40.731419468Z"
    },
    "id": "EaBgGTIz72ci"
   },
   "outputs": [],
   "source": [
    "@builtin.module(attrs={\"transform.with_named_sequence\": UnitAttr.get()})\n",
    "def mod_transform():\n",
    "    @named_sequence(\n",
    "        \"main\", [any_op_t()], [], arg_attrs=[{\"transform.readonly\": UnitAttr.get()}]\n",
    "    )\n",
    "    def main(module: any_op_t()):\n",
    "        matmul = transform.match(module, [\"linalg.matmul\"])\n",
    "        transform.nvgpu.rewrite_matmul_as_mma_sync(matmul)\n",
    "        # clean up to simplify test below...\n",
    "        all_loops = transform.match(\n",
    "            module, interface=MatchInterfaceEnum.LoopLikeInterface\n",
    "        )\n",
    "        transform.apply_licm(all_loops)\n",
    "        transform.apply_cse(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADbabroS8ND2"
   },
   "source": [
    "# \"Finish\" the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.775039674Z",
     "start_time": "2024-02-02T23:42:40.733656110Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUOsYXaW8QKC",
    "outputId": "f8592229-1d9b-4c52-9133-30fd52c2716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module {\n",
      "  module attributes {transform.target_tag = \"payload\"} {\n",
      "    func.func @compute_linspace_val(%arg0: index, %arg1: index, %arg2: index) -> f16 {\n",
      "      %0 = arith.index_cast %arg0 : index to i32\n",
      "      %1 = arith.index_cast %arg1 : index to i32\n",
      "      %2 = arith.index_cast %arg2 : index to i32\n",
      "      %3 = arith.muli %0, %2 : i32\n",
      "      %4 = arith.addi %1, %3 : i32\n",
      "      %5 = arith.sitofp %4 : i32 to f16\n",
      "      %cst = arith.constant 6.400000e+01 : f16\n",
      "      %6 = arith.divf %5, %cst : f16\n",
      "      return %6 : f16\n",
      "    }\n",
      "    func.func private @printMemrefF32(memref<*xf32>)\n",
      "    func.func @print_lhs_as_memref_32(%arg0: memref<16x16xf16>) {\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %dim = memref.dim %arg0, %c0 : memref<16x16xf16>\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim_0 = memref.dim %arg0, %c1 : memref<16x16xf16>\n",
      "      %alloc = memref.alloc(%dim, %dim_0) : memref<?x?xf32>\n",
      "      %c0_1 = arith.constant 0 : index\n",
      "      %c1_2 = arith.constant 1 : index\n",
      "      scf.for %arg1 = %c0_1 to %dim step %c1_2 {\n",
      "        %c0_3 = arith.constant 0 : index\n",
      "        %c1_4 = arith.constant 1 : index\n",
      "        scf.for %arg2 = %c0_3 to %dim_0 step %c1_4 {\n",
      "          %0 = memref.load %arg0[%arg1, %arg2] : memref<16x16xf16>\n",
      "          %1 = arith.extf %0 : f16 to f32\n",
      "          memref.store %1, %alloc[%arg1, %arg2] : memref<?x?xf32>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<?x?xf32> to memref<*xf32>\n",
      "      call @printMemrefF32(%cast) : (memref<*xf32>) -> ()\n",
      "      memref.dealloc %alloc : memref<?x?xf32>\n",
      "      return\n",
      "    }\n",
      "    func.func @print_rhs_as_memref_32(%arg0: memref<16x8xf16>) {\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %dim = memref.dim %arg0, %c0 : memref<16x8xf16>\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim_0 = memref.dim %arg0, %c1 : memref<16x8xf16>\n",
      "      %alloc = memref.alloc(%dim, %dim_0) : memref<?x?xf32>\n",
      "      %c0_1 = arith.constant 0 : index\n",
      "      %c1_2 = arith.constant 1 : index\n",
      "      scf.for %arg1 = %c0_1 to %dim step %c1_2 {\n",
      "        %c0_3 = arith.constant 0 : index\n",
      "        %c1_4 = arith.constant 1 : index\n",
      "        scf.for %arg2 = %c0_3 to %dim_0 step %c1_4 {\n",
      "          %0 = memref.load %arg0[%arg1, %arg2] : memref<16x8xf16>\n",
      "          %1 = arith.extf %0 : f16 to f32\n",
      "          memref.store %1, %alloc[%arg1, %arg2] : memref<?x?xf32>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<?x?xf32> to memref<*xf32>\n",
      "      call @printMemrefF32(%cast) : (memref<*xf32>) -> ()\n",
      "      memref.dealloc %alloc : memref<?x?xf32>\n",
      "      return\n",
      "    }\n",
      "    func.func @print_res_as_memref_32(%arg0: memref<16x8xf16>) {\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim = memref.dim %arg0, %c0 : memref<16x8xf16>\n",
      "      %dim_0 = memref.dim %arg0, %c1 : memref<16x8xf16>\n",
      "      %alloc = memref.alloc(%dim, %dim_0) : memref<?x?xf32>\n",
      "      %c0_1 = arith.constant 0 : index\n",
      "      %c1_2 = arith.constant 1 : index\n",
      "      scf.for %arg1 = %c0_1 to %dim step %c1_2 {\n",
      "        %c0_3 = arith.constant 0 : index\n",
      "        %c1_4 = arith.constant 1 : index\n",
      "        scf.for %arg2 = %c0_3 to %dim_0 step %c1_4 {\n",
      "          %0 = memref.load %arg0[%arg1, %arg2] : memref<16x8xf16>\n",
      "          %1 = arith.extf %0 : f16 to f32\n",
      "          memref.store %1, %alloc[%arg1, %arg2] : memref<?x?xf32>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<?x?xf32> to memref<*xf32>\n",
      "      call @printMemrefF32(%cast) : (memref<*xf32>) -> ()\n",
      "      memref.dealloc %alloc : memref<?x?xf32>\n",
      "      return\n",
      "    }\n",
      "    func.func @main() {\n",
      "      %alloc = memref.alloc() : memref<16x16xf16>\n",
      "      %alloc_0 = memref.alloc() : memref<16x8xf16>\n",
      "      %alloc_1 = memref.alloc() : memref<16x8xf16>\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %dim = memref.dim %alloc_1, %c0 : memref<16x8xf16>\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim_2 = memref.dim %alloc_1, %c1 : memref<16x8xf16>\n",
      "      %c1_3 = arith.constant 1 : index\n",
      "      %dim_4 = memref.dim %alloc, %c1_3 : memref<16x16xf16>\n",
      "      %cst = arith.constant 1.000000e+00 : f16\n",
      "      %cst_5 = arith.constant 0.000000e+00 : f16\n",
      "      %c32 = arith.constant 32 : index\n",
      "      %c0_6 = arith.constant 0 : index\n",
      "      %c1_7 = arith.constant 1 : index\n",
      "      scf.for %arg0 = %c0_6 to %dim step %c1_7 {\n",
      "        %c0_20 = arith.constant 0 : index\n",
      "        %c1_21 = arith.constant 1 : index\n",
      "        scf.for %arg1 = %c0_20 to %dim_4 step %c1_21 {\n",
      "          %0 = func.call @compute_linspace_val(%arg0, %arg1, %dim_4) : (index, index, index) -> f16\n",
      "          memref.store %0, %alloc[%arg0, %arg1] : memref<16x16xf16>\n",
      "        }\n",
      "      }\n",
      "      %c0_8 = arith.constant 0 : index\n",
      "      %c1_9 = arith.constant 1 : index\n",
      "      scf.for %arg0 = %c0_8 to %dim_4 step %c1_9 {\n",
      "        %c0_20 = arith.constant 0 : index\n",
      "        %c1_21 = arith.constant 1 : index\n",
      "        scf.for %arg1 = %c0_20 to %dim_2 step %c1_21 {\n",
      "          %0 = func.call @compute_linspace_val(%arg0, %arg1, %dim_2) : (index, index, index) -> f16\n",
      "          memref.store %0, %alloc_0[%arg0, %arg1] : memref<16x8xf16>\n",
      "        }\n",
      "      }\n",
      "      %c0_10 = arith.constant 0 : index\n",
      "      %c1_11 = arith.constant 1 : index\n",
      "      scf.for %arg0 = %c0_10 to %dim step %c1_11 {\n",
      "        %c0_20 = arith.constant 0 : index\n",
      "        %c1_21 = arith.constant 1 : index\n",
      "        scf.for %arg1 = %c0_20 to %dim_2 step %c1_21 {\n",
      "          %0 = func.call @compute_linspace_val(%arg0, %arg1, %dim_2) : (index, index, index) -> f16\n",
      "          memref.store %0, %alloc_1[%arg0, %arg1] : memref<16x8xf16>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<16x16xf16> to memref<*xf16>\n",
      "      %cast_12 = memref.cast %alloc_0 : memref<16x8xf16> to memref<*xf16>\n",
      "      %cast_13 = memref.cast %alloc_1 : memref<16x8xf16> to memref<*xf16>\n",
      "      gpu.host_register %cast : memref<*xf16>\n",
      "      gpu.host_register %cast_12 : memref<*xf16>\n",
      "      gpu.host_register %cast_13 : memref<*xf16>\n",
      "      call @print_lhs_as_memref_32(%alloc) : (memref<16x16xf16>) -> ()\n",
      "      call @print_rhs_as_memref_32(%alloc_0) : (memref<16x8xf16>) -> ()\n",
      "      %c1_14 = arith.constant 1 : index\n",
      "      %c1_15 = arith.constant 1 : index\n",
      "      %c1_16 = arith.constant 1 : index\n",
      "      %c32_17 = arith.constant 32 : index\n",
      "      %c1_18 = arith.constant 1 : index\n",
      "      %c1_19 = arith.constant 1 : index\n",
      "      gpu.launch blocks(%arg0, %arg1, %arg2) in (%arg6 = %c1_14, %arg7 = %c1_15, %arg8 = %c1_16) threads(%arg3, %arg4, %arg5) in (%arg9 = %c32_17, %arg10 = %c1_18, %arg11 = %c1_19) {\n",
      "        linalg.matmul {cast = #linalg.type_fn<cast_signed>} ins(%alloc, %alloc_0 : memref<16x16xf16>, memref<16x8xf16>) outs(%alloc_1 : memref<16x8xf16>)\n",
      "        gpu.terminator\n",
      "      }\n",
      "      call @print_res_as_memref_32(%alloc_1) : (memref<16x8xf16>) -> ()\n",
      "      return\n",
      "    }\n",
      "  }\n",
      "  module attributes {transform.with_named_sequence} {\n",
      "    transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly}) {\n",
      "      %0 = transform.structured.match ops{[\"linalg.matmul\"]} in %arg0 : (!transform.any_op) -> !transform.any_op\n",
      "      transform.nvgpu.rewrite_matmul_as_mma_sync %0 : (!transform.any_op) -> ()\n",
      "      %1 = transform.structured.match interface{LoopLikeInterface} in %arg0 : (!transform.any_op) -> !transform.any_op\n",
      "      transform.apply_licm to %1 : !transform.any_op\n",
      "      transform.apply_cse to %arg0 : !transform.any_op\n",
      "      transform.yield \n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "module = module.finish()\n",
    "print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xN5kNvZ8Tyf"
   },
   "source": [
    "# Execute the transform schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.775757596Z",
     "start_time": "2024-02-02T23:42:40.774788838Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLwQLPD98Q4d",
    "outputId": "ecfa6c9a-15eb-40c7-df29-f43fcac02fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#map = affine_map<(d0) -> (d0 floordiv 4)>\n",
      "#map1 = affine_map<(d0) -> (d0 * 2 - (d0 floordiv 4) * 8)>\n",
      "#map2 = affine_map<(d0) -> (d0 * 2 - (d0 floordiv 4) * 8 + 1)>\n",
      "#map3 = affine_map<(d0) -> (d0 floordiv 4 + 8)>\n",
      "#map4 = affine_map<(d0) -> (d0 * 2 - (d0 floordiv 4) * 8 + 8)>\n",
      "#map5 = affine_map<(d0) -> (d0 * 2 - (d0 floordiv 4) * 8 + 9)>\n",
      "module {\n",
      "  module attributes {transform.target_tag = \"payload\"} {\n",
      "    func.func @compute_linspace_val(%arg0: index, %arg1: index, %arg2: index) -> f16 {\n",
      "      %0 = arith.index_cast %arg0 : index to i32\n",
      "      %1 = arith.index_cast %arg1 : index to i32\n",
      "      %2 = arith.index_cast %arg2 : index to i32\n",
      "      %3 = arith.muli %0, %2 : i32\n",
      "      %4 = arith.addi %1, %3 : i32\n",
      "      %5 = arith.sitofp %4 : i32 to f16\n",
      "      %cst = arith.constant 6.400000e+01 : f16\n",
      "      %6 = arith.divf %5, %cst : f16\n",
      "      return %6 : f16\n",
      "    }\n",
      "    func.func private @printMemrefF32(memref<*xf32>)\n",
      "    func.func @print_lhs_as_memref_32(%arg0: memref<16x16xf16>) {\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %dim = memref.dim %arg0, %c0 : memref<16x16xf16>\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim_0 = memref.dim %arg0, %c1 : memref<16x16xf16>\n",
      "      %alloc = memref.alloc(%dim, %dim_0) : memref<?x?xf32>\n",
      "      scf.for %arg1 = %c0 to %dim step %c1 {\n",
      "        scf.for %arg2 = %c0 to %dim_0 step %c1 {\n",
      "          %0 = memref.load %arg0[%arg1, %arg2] : memref<16x16xf16>\n",
      "          %1 = arith.extf %0 : f16 to f32\n",
      "          memref.store %1, %alloc[%arg1, %arg2] : memref<?x?xf32>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<?x?xf32> to memref<*xf32>\n",
      "      call @printMemrefF32(%cast) : (memref<*xf32>) -> ()\n",
      "      memref.dealloc %alloc : memref<?x?xf32>\n",
      "      return\n",
      "    }\n",
      "    func.func @print_rhs_as_memref_32(%arg0: memref<16x8xf16>) {\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %dim = memref.dim %arg0, %c0 : memref<16x8xf16>\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim_0 = memref.dim %arg0, %c1 : memref<16x8xf16>\n",
      "      %alloc = memref.alloc(%dim, %dim_0) : memref<?x?xf32>\n",
      "      scf.for %arg1 = %c0 to %dim step %c1 {\n",
      "        scf.for %arg2 = %c0 to %dim_0 step %c1 {\n",
      "          %0 = memref.load %arg0[%arg1, %arg2] : memref<16x8xf16>\n",
      "          %1 = arith.extf %0 : f16 to f32\n",
      "          memref.store %1, %alloc[%arg1, %arg2] : memref<?x?xf32>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<?x?xf32> to memref<*xf32>\n",
      "      call @printMemrefF32(%cast) : (memref<*xf32>) -> ()\n",
      "      memref.dealloc %alloc : memref<?x?xf32>\n",
      "      return\n",
      "    }\n",
      "    func.func @print_res_as_memref_32(%arg0: memref<16x8xf16>) {\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim = memref.dim %arg0, %c0 : memref<16x8xf16>\n",
      "      %dim_0 = memref.dim %arg0, %c1 : memref<16x8xf16>\n",
      "      %alloc = memref.alloc(%dim, %dim_0) : memref<?x?xf32>\n",
      "      scf.for %arg1 = %c0 to %dim step %c1 {\n",
      "        scf.for %arg2 = %c0 to %dim_0 step %c1 {\n",
      "          %0 = memref.load %arg0[%arg1, %arg2] : memref<16x8xf16>\n",
      "          %1 = arith.extf %0 : f16 to f32\n",
      "          memref.store %1, %alloc[%arg1, %arg2] : memref<?x?xf32>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<?x?xf32> to memref<*xf32>\n",
      "      call @printMemrefF32(%cast) : (memref<*xf32>) -> ()\n",
      "      memref.dealloc %alloc : memref<?x?xf32>\n",
      "      return\n",
      "    }\n",
      "    func.func @main() {\n",
      "      %alloc = memref.alloc() : memref<16x16xf16>\n",
      "      %alloc_0 = memref.alloc() : memref<16x8xf16>\n",
      "      %alloc_1 = memref.alloc() : memref<16x8xf16>\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %dim = memref.dim %alloc_1, %c0 : memref<16x8xf16>\n",
      "      %c1 = arith.constant 1 : index\n",
      "      %dim_2 = memref.dim %alloc_1, %c1 : memref<16x8xf16>\n",
      "      %dim_3 = memref.dim %alloc, %c1 : memref<16x16xf16>\n",
      "      scf.for %arg0 = %c0 to %dim step %c1 {\n",
      "        scf.for %arg1 = %c0 to %dim_3 step %c1 {\n",
      "          %0 = func.call @compute_linspace_val(%arg0, %arg1, %dim_3) : (index, index, index) -> f16\n",
      "          memref.store %0, %alloc[%arg0, %arg1] : memref<16x16xf16>\n",
      "        }\n",
      "      }\n",
      "      scf.for %arg0 = %c0 to %dim_3 step %c1 {\n",
      "        scf.for %arg1 = %c0 to %dim_2 step %c1 {\n",
      "          %0 = func.call @compute_linspace_val(%arg0, %arg1, %dim_2) : (index, index, index) -> f16\n",
      "          memref.store %0, %alloc_0[%arg0, %arg1] : memref<16x8xf16>\n",
      "        }\n",
      "      }\n",
      "      scf.for %arg0 = %c0 to %dim step %c1 {\n",
      "        scf.for %arg1 = %c0 to %dim_2 step %c1 {\n",
      "          %0 = func.call @compute_linspace_val(%arg0, %arg1, %dim_2) : (index, index, index) -> f16\n",
      "          memref.store %0, %alloc_1[%arg0, %arg1] : memref<16x8xf16>\n",
      "        }\n",
      "      }\n",
      "      %cast = memref.cast %alloc : memref<16x16xf16> to memref<*xf16>\n",
      "      %cast_4 = memref.cast %alloc_0 : memref<16x8xf16> to memref<*xf16>\n",
      "      %cast_5 = memref.cast %alloc_1 : memref<16x8xf16> to memref<*xf16>\n",
      "      gpu.host_register %cast : memref<*xf16>\n",
      "      gpu.host_register %cast_4 : memref<*xf16>\n",
      "      gpu.host_register %cast_5 : memref<*xf16>\n",
      "      call @print_lhs_as_memref_32(%alloc) : (memref<16x16xf16>) -> ()\n",
      "      call @print_rhs_as_memref_32(%alloc_0) : (memref<16x8xf16>) -> ()\n",
      "      %c32 = arith.constant 32 : index\n",
      "      gpu.launch blocks(%arg0, %arg1, %arg2) in (%arg6 = %c1, %arg7 = %c1, %arg8 = %c1) threads(%arg3, %arg4, %arg5) in (%arg9 = %c32, %arg10 = %c1, %arg11 = %c1) {\n",
      "        %thread_id_x = gpu.thread_id  x\n",
      "        %0 = affine.apply #map(%thread_id_x)\n",
      "        %1 = affine.apply #map1(%thread_id_x)\n",
      "        %2 = memref.load %alloc[%0, %1] : memref<16x16xf16>\n",
      "        %3 = affine.apply #map2(%thread_id_x)\n",
      "        %4 = memref.load %alloc[%0, %3] : memref<16x16xf16>\n",
      "        %5 = affine.apply #map3(%thread_id_x)\n",
      "        %6 = memref.load %alloc[%5, %1] : memref<16x16xf16>\n",
      "        %7 = memref.load %alloc[%5, %3] : memref<16x16xf16>\n",
      "        %8 = affine.apply #map4(%thread_id_x)\n",
      "        %9 = memref.load %alloc[%0, %8] : memref<16x16xf16>\n",
      "        %10 = affine.apply #map5(%thread_id_x)\n",
      "        %11 = memref.load %alloc[%0, %10] : memref<16x16xf16>\n",
      "        %12 = memref.load %alloc[%5, %8] : memref<16x16xf16>\n",
      "        %13 = memref.load %alloc[%5, %10] : memref<16x16xf16>\n",
      "        %14 = vector.splat %2 : vector<4x2xf16>\n",
      "        %15 = vector.insert %2, %14 [0, 0] : f16 into vector<4x2xf16>\n",
      "        %16 = vector.insert %4, %15 [0, 1] : f16 into vector<4x2xf16>\n",
      "        %17 = vector.insert %6, %16 [1, 0] : f16 into vector<4x2xf16>\n",
      "        %18 = vector.insert %7, %17 [1, 1] : f16 into vector<4x2xf16>\n",
      "        %19 = vector.insert %9, %18 [2, 0] : f16 into vector<4x2xf16>\n",
      "        %20 = vector.insert %11, %19 [2, 1] : f16 into vector<4x2xf16>\n",
      "        %21 = vector.insert %12, %20 [3, 0] : f16 into vector<4x2xf16>\n",
      "        %22 = vector.insert %13, %21 [3, 1] : f16 into vector<4x2xf16>\n",
      "        %23 = memref.load %alloc_0[%1, %0] : memref<16x8xf16>\n",
      "        %24 = memref.load %alloc_0[%3, %0] : memref<16x8xf16>\n",
      "        %25 = memref.load %alloc_0[%8, %0] : memref<16x8xf16>\n",
      "        %26 = memref.load %alloc_0[%10, %0] : memref<16x8xf16>\n",
      "        %27 = vector.splat %23 : vector<2x2xf16>\n",
      "        %28 = vector.insert %23, %27 [0, 0] : f16 into vector<2x2xf16>\n",
      "        %29 = vector.insert %24, %28 [0, 1] : f16 into vector<2x2xf16>\n",
      "        %30 = vector.insert %25, %29 [1, 0] : f16 into vector<2x2xf16>\n",
      "        %31 = vector.insert %26, %30 [1, 1] : f16 into vector<2x2xf16>\n",
      "        %32 = memref.load %alloc_1[%0, %1] : memref<16x8xf16>\n",
      "        %33 = memref.load %alloc_1[%0, %3] : memref<16x8xf16>\n",
      "        %34 = memref.load %alloc_1[%5, %1] : memref<16x8xf16>\n",
      "        %35 = memref.load %alloc_1[%5, %3] : memref<16x8xf16>\n",
      "        %36 = vector.splat %32 : vector<2x2xf16>\n",
      "        %37 = vector.insert %32, %36 [0, 0] : f16 into vector<2x2xf16>\n",
      "        %38 = vector.insert %33, %37 [0, 1] : f16 into vector<2x2xf16>\n",
      "        %39 = vector.insert %34, %38 [1, 0] : f16 into vector<2x2xf16>\n",
      "        %40 = vector.insert %35, %39 [1, 1] : f16 into vector<2x2xf16>\n",
      "        %41 = nvgpu.mma.sync(%22, %31, %40) {mmaShape = [16, 8, 16]} : (vector<4x2xf16>, vector<2x2xf16>, vector<2x2xf16>) -> vector<2x2xf16>\n",
      "        %42 = vector.extract %41[0, 0] : f16 from vector<2x2xf16>\n",
      "        %43 = vector.extract %41[0, 1] : f16 from vector<2x2xf16>\n",
      "        %44 = vector.extract %41[1, 0] : f16 from vector<2x2xf16>\n",
      "        %45 = vector.extract %41[1, 1] : f16 from vector<2x2xf16>\n",
      "        memref.store %42, %alloc_1[%0, %1] : memref<16x8xf16>\n",
      "        memref.store %43, %alloc_1[%0, %3] : memref<16x8xf16>\n",
      "        memref.store %44, %alloc_1[%5, %1] : memref<16x8xf16>\n",
      "        memref.store %45, %alloc_1[%5, %3] : memref<16x8xf16>\n",
      "        gpu.terminator\n",
      "      }\n",
      "      call @print_res_as_memref_32(%alloc_1) : (memref<16x8xf16>) -> ()\n",
      "      return\n",
      "    }\n",
      "  }\n",
      "  module attributes {transform.with_named_sequence} {\n",
      "    transform.named_sequence @main(%arg0: !transform.any_op {transform.readonly}) {\n",
      "      %0 = transform.structured.match ops{[\"linalg.matmul\"]} in %arg0 : (!transform.any_op) -> !transform.any_op\n",
      "      transform.nvgpu.rewrite_matmul_as_mma_sync %0 : (!transform.any_op) -> ()\n",
      "      %1 = transform.structured.match interface{LoopLikeInterface} in %arg0 : (!transform.any_op) -> !transform.any_op\n",
      "      transform.apply_licm to %1 : !transform.any_op\n",
      "      transform.apply_cse to %arg0 : !transform.any_op\n",
      "      transform.yield \n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = run_pipeline(\n",
    "    module,\n",
    "    Pipeline().transform_interpreter(\n",
    "        entry_point=\"main\", debug_payload_root_tag=\"payload\"\n",
    "    ),\n",
    ")\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_NURglF8ZZW"
   },
   "source": [
    "# Lower to NVVM (and LLVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.830771046Z",
     "start_time": "2024-02-02T23:42:40.775057708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IoWjgc48bcn",
    "outputId": "39550464-fd37-4e6d-a257-e803b746d8de"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "backend = LLVMJITBackend([CUDA_RUNTIME_LIB_PATH])\n",
    "# this doesn't actually anything (no pipeline) but does generate C API/wrappers\n",
    "compiled_module = backend.compile(\n",
    "    find_ops(\n",
    "        mod.operation,\n",
    "        lambda x: \"transform.target_tag\" in x.attributes\n",
    "                  and x.attributes[\"transform.target_tag\"].value == \"payload\",\n",
    "        single=True,\n",
    "    ),\n",
    "    Pipeline().add_pass(\n",
    "        \"gpu-lower-to-nvvm-pipeline\",\n",
    "        **{\n",
    "            \"cubin-chip\": \"sm_75\",\n",
    "            \"cubin-features\": \"+ptx75\",\n",
    "            \"cubin-format\": \"fatbin\",\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "print(compiled_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOapyydH8n4h"
   },
   "source": [
    "# Load and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T23:42:40.965581098Z",
     "start_time": "2024-02-02T23:42:40.831596632Z"
    },
    "id": "pOEC4Qgw8p9X"
   },
   "outputs": [],
   "source": [
    "backend.load(compiled_module).main_capi_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
